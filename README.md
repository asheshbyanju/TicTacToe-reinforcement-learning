# Battle Between Two Tic-Tac-Toe Reinforcement Agents
Tic tac toe is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3Ã—3 grid. The player who succeeds in placing three of their marks in a diagonal, horizontal, or vertical row is the winner. <br />
In this project, I intended to develop a agent who will learn how to play tic tac toe and compete again other players. Main objective of the project is to train the agent to learn the strategy to find the best available move and beat Random Agent. I will be using reinforcement Q-Learning to achieve goal.

![tic](/Tic_tac_toe.png)


## Overview
* Used two strategies: (Random and Q-Learning) 
* Played 4 games: Random vs Random, Random vs Q-learning, Q-learning vs Random and Q-Learning vs Q-Learning
* Played 10,000 games using each strategy and after the games are played gathered our statistics and plot the results to see how Q-Learning agent performed.<br>

#### Tools and libraries used
* Python 3.7
* Spyder
* Pandas
* Matplotlib

#### Requirements
Codes are writtern in python and requires python 3.6 + to run.

## Q-Learning Implementation
* We need to store all of the states our agent experiences during its lifetime in order for the Q-Learning algorithm to work. 
* This allows our agent to remember which action is performed in each state, so that the next time he experiences the same state, it will perform the same action. This is our agent's memory
* State field is a map with the key being a hash of our board (a unique string formed from the board configuration) and the value being the Q-value of that board configuration, which is the value calculated using the Bellman equation.
* The states history field is the final field in our class, and it contains the hashes of all the board configurations encountered by the player in the last game.
* After the game is over, the model will retrace the steps in the history and generate new q-values for the next game. 
* Another very important method is the reward method which is named at the end of each game. 
* This will take the positive or negative reward (depending on the game winner) and change the Q values for the board configuration we saw in the previous game using the Bellman equation.

![Q-Learning](/Q-Learning-algorithm1.png)


## Running code
Instructions on how to run the project:<br>
**Step 1:** Download the zip file or clone the repository <br>
**Step 2:** cd to the directory where your downloaded folder is located.<br>
**Step 3:** run: `pip install -r requirements.txt` in your shell <br>
**Step 4:** open the project folder in spyder <br>
**Step 5:** open main.py and run

## Results
Q-Learning Player vs Q-Learning Player <br>
==================== <br>
Train result - 10000 games <br>
Q-Learning win: 2322 (23%) <br>
Q-Learning win: 691 (6%) <br>
Draw: 6986 (69%) <br> 
==================== <br><br>
Q-Learning Player vs Random Player <br>
==================== <br>
Train result - 10000 games <br>
Q-Learning win: 9197 (91%) <br>
Random win: 397 (3%) <br>
Draw: 405 (4%) <br>
==================== <br>

